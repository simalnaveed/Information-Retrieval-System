{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Traverse Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign directory\n",
    "directory = 'Corpus'\n",
    "\n",
    "files = os.listdir(directory) # list conataining the names of files present in the argumented directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = {}               # files_dic to store index of every file\n",
    "files_path = {}               # dictionary to store path of every file\n",
    "fcount = 0                    # file_count to count number of files\n",
    "\n",
    "# iterate over files in that directory and fill the dictionaries accordingly\n",
    "for filename in files:\n",
    "    files_dict[filename] = fcount                 \n",
    "    files_path[filename] = directory + '/' + filename \n",
    "    fcount = fcount + 1\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number  of files:  3\n",
      "\n",
      "Dictionary containing  files:\n",
      " {'file1.txt': 0, 'file2.txt': 1, 'file3.txt': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTotal Number  of files: \", fcount)\n",
    "print(\"\\nDictionary containing  files:\\n\", files_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Extract Unique Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary containing unique words:\n",
      " {'a': 0, 'on': 1, 'procedural': 2, 'by': 3, 'be': 4, 'language,': 5, 'emphasis': 6, 'its': 7, 'high-performance': 8, 'design': 9, 'is': 10, 'used': 11, 'use': 12, 'that': 13, 'create': 14, 'developed': 15, 'which': 16, 'objects.': 17, 'rule.': 18, 'philosophy': 19, 'difference': 20, 'code': 21, 'bjarne': 22, 'via': 23, 'programming': 24, 'applications.': 25, 'extension': 26, 'indentation': 27, 'of': 28, 'based': 29, 'c++': 30, 'besides': 31, 'high-level,': 32, 'stroustrup,': 33, 'language': 34, 'the': 35, 'an': 36, 'general-purpose': 37, 'diving': 38, 'to': 39, 'into': 40, 'emphasizes': 41, 'readability': 42, 'cross-platform': 43, 'can': 44, 'c': 45, 'off-side': 46, 'as': 47, 'being': 48, 'between': 49, 'concept': 50, 'python': 51, 'and': 52, 'oriented': 53, 'object': 54, 'was': 55, 'cpp': 56, 'significant': 57, 'program': 58, 'functions.': 59, 'also': 60, 'language.': 61, 'with': 62}\n"
     ]
    }
   ],
   "source": [
    "unique_word_count = 0      # varaible to count the number of unique words\n",
    "unique_word_dict = {}      # unique_word_dict to store the indices of the unique words \n",
    "unique_word_set = set()    # unique_word_set to store all the words uniquely\n",
    "\n",
    "# iterate over files to uniquely extract and store the words inside them  \n",
    "for filename in files:\n",
    "    tempf = open(files_path[filename])\n",
    "    temp = tempf.read().lower()\n",
    "    words = temp.split()\n",
    "    unique_word_set = unique_word_set | set(words)\n",
    "\n",
    "unique_word_list = list(unique_word_set)  # converting the set into the list to assign indices to the words \n",
    "\n",
    "# storing the indices in word dictionary\n",
    "for word in unique_word_list:\n",
    "    unique_word_dict[word] = unique_word_count \n",
    "    unique_word_count = unique_word_count + 1\n",
    "\n",
    "print('Dictionary containing unique words:\\n',unique_word_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create Term Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "TDM = np.zeros((fcount , unique_word_count))\n",
    "print( 'TDM:\\n',TDM)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Fill Term Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM(filled):\n",
      " [[1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1.\n",
      "  1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      "  1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      "  1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# iterate over the words in all the files to fill TDM  \n",
    "for filename in files_dict:\n",
    "    tempf = open(files_path[filename])\n",
    "    temp = tempf.read().lower()\n",
    "    words = temp.split()\n",
    "\n",
    "    # to iterate over all the words in a single file\n",
    "    for word in words:   \n",
    "        #if word in the file i exists in word list at j ,then place 1 at TDM[i][j]\n",
    "        if word in unique_word_list:\n",
    "            TDM[files_dict[filename]][unique_word_dict[word]] = 1\n",
    "\n",
    "print( 'TDM(filled):\\n',TDM)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Ask for a user Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query vector:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "query = input(\"\\nWrite something for searching: \")\n",
    "query_vector = np.zeros((len(unique_word_set),1))\n",
    "query = query.lower()\n",
    "words = query.split()\n",
    "words_set = set(words) # to avoid duplication of words in the query\n",
    "\n",
    "# iterate over all the unique words present in the query\n",
    "for word in words_set:\n",
    "        #if word in the query exists in word list at i ,then place 1 at query_vector[i][0]\n",
    "        if word in unique_word_list:\n",
    "            query_vector[unique_word_dict[word]][0] = 1\n",
    "print(\"query vector:\\n\",query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 Display Resultant Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultant vector: \n",
      " [[1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "resultant = np.dot(TDM, query_vector)\n",
    "print(\"resultant vector: \\n\",resultant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Display the contents of file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number in resultant vector is 1.0 at index 0\n"
     ]
    }
   ],
   "source": [
    "max =  resultant.max() # getting the max number\n",
    "index = -1\n",
    "# loop to find trhe index of resultant vector at which max number is placed\n",
    "for element in resultant:\n",
    "    index = index + 1\n",
    "    if element == max:\n",
    "        break\n",
    "\n",
    "print(\"Max number in resultant vector is\",max,\"at index\",index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      " Python language\n",
      "Python is a high-level, general-purpose programming language. \n",
      "Its design philosophy emphasizes code readability with the use of significant indentation via the off-side rule.\n"
     ]
    }
   ],
   "source": [
    "ResFile = open(files_path[files[index]]) \n",
    "fileData = ResFile.read()\n",
    "print(\"Results:\\n\",fileData)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
